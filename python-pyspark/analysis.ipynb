{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4036efae-94ed-4f5d-91dc-452be631897f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_source_1 DATA_SOURCE_1]\n",
      "                             [--data_source_2 DATA_SOURCE_2]\n",
      "                             [--output_uri OUTPUT_URI]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/johnpaulbabu/Library/Jupyter/runtime/kernel-e7f4dbee-914e-454b-86cb-0d289614be19.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "def merge_two_files(data_source_1,data_source_2):\n",
    "    \"\"\"\n",
    "    Merge 2 files into 1\n",
    "    \n",
    "    Usage \n",
    "    \n",
    "    !spark-submit analysis.py --data_source_1 x_list.txt --data_source_2 y_list.txt\n",
    "    \"\"\"\n",
    "    spark=SparkSession.builder.appName(\"merge-two-files\").getOrCreate()\n",
    "    \n",
    "    x_schema = StructType([\n",
    "    StructField(\"ID_x\", StringType(), True),\n",
    "    StructField(\"value_x\", StringType(), True)])\n",
    "    \n",
    "    y_schema = StructType([\n",
    "    StructField(\"ID_y\", StringType(), True),\n",
    "    StructField(\"value_y\", StringType(), True)])\n",
    "    \n",
    "    if data_source_1 is not None:\n",
    "      x = spark.read.csv(data_source_1, sep='\\t',header=False,schema=x_schema)\n",
    "    \n",
    "    if data_source_2 is not None:\n",
    "      y = spark.read.csv(data_source_2, sep='\\t', header=False, schema=y_schema)\n",
    "    \n",
    "    res = x.join(y, x.ID_x == y.ID_y, how= \"left\")\n",
    "    res1 = res.drop(res.ID_y)\n",
    "    res1.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--data_source_1', help=\"The URI for you CSV data, like an S3 bucket location.\")\n",
    "    parser.add_argument(\n",
    "        '--data_source_2', help=\"The URI for you CSV data, like an S3 bucket location.\")\n",
    "    parser.add_argument(\n",
    "        '--output_uri', help=\"The URI where output is saved, like an S3 bucket location.\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "\n",
    "    merge_two_files(args.data_source_1, args.data_source_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a75357eb-3f0c-40ab-85ef-bad5ecfb43f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/13 17:06:45 WARN Utils: Your hostname, johns-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.14 instead (on interface en0)\n",
      "23/01/13 17:06:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/01/13 17:06:47 INFO SparkContext: Running Spark version 3.3.1\n",
      "23/01/13 17:06:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/13 17:06:47 INFO ResourceUtils: ==============================================================\n",
      "23/01/13 17:06:47 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/01/13 17:06:47 INFO ResourceUtils: ==============================================================\n",
      "23/01/13 17:06:47 INFO SparkContext: Submitted application: merge-two-files\n",
      "23/01/13 17:06:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/01/13 17:06:47 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/01/13 17:06:47 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/01/13 17:06:47 INFO SecurityManager: Changing view acls to: johnpaulbabu\n",
      "23/01/13 17:06:47 INFO SecurityManager: Changing modify acls to: johnpaulbabu\n",
      "23/01/13 17:06:47 INFO SecurityManager: Changing view acls groups to: \n",
      "23/01/13 17:06:47 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/01/13 17:06:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(johnpaulbabu); groups with view permissions: Set(); users  with modify permissions: Set(johnpaulbabu); groups with modify permissions: Set()\n",
      "23/01/13 17:06:48 INFO Utils: Successfully started service 'sparkDriver' on port 49248.\n",
      "23/01/13 17:06:48 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/01/13 17:06:48 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/01/13 17:06:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/01/13 17:06:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/01/13 17:06:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/01/13 17:06:48 INFO DiskBlockManager: Created local directory at /private/var/folders/8_/fxpby0zx7dl6sv253yrcxb0r0000gn/T/blockmgr-1e59bd55-1c03-4bc7-8516-94cddc3f0391\n",
      "23/01/13 17:06:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "23/01/13 17:06:48 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/01/13 17:06:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/01/13 17:06:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/01/13 17:06:49 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/01/13 17:06:49 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/01/13 17:06:49 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/01/13 17:06:49 INFO Utils: Successfully started service 'SparkUI' on port 4045.\n",
      "23/01/13 17:06:49 INFO Executor: Starting executor ID driver on host 192.168.0.14\n",
      "23/01/13 17:06:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/01/13 17:06:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49250.\n",
      "23/01/13 17:06:49 INFO NettyBlockTransferService: Server created on 192.168.0.14:49250\n",
      "23/01/13 17:06:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/01/13 17:06:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.14, 49250, None)\n",
      "23/01/13 17:06:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.14:49250 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.14, 49250, None)\n",
      "23/01/13 17:06:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.14, 49250, None)\n",
      "23/01/13 17:06:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.14, 49250, None)\n",
      "23/01/13 17:06:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/01/13 17:06:50 INFO SharedState: Warehouse path is 'file:/Users/johnpaulbabu/Documents/DevOps/pyspark/spark-warehouse'.\n",
      "23/01/13 17:06:51 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.\n",
      "23/01/13 17:06:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "23/01/13 17:06:53 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/01/13 17:06:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/01/13 17:06:53 INFO FileSourceStrategy: Output Data Schema: struct<ID_x: string, value_x: string>\n",
      "23/01/13 17:06:53 INFO FileSourceStrategy: Pushed Filters: IsNotNull(ID_y)\n",
      "23/01/13 17:06:53 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(ID_y#4)\n",
      "23/01/13 17:06:53 INFO FileSourceStrategy: Output Data Schema: struct<ID_y: string, value_y: string>\n",
      "23/01/13 17:06:54 INFO CodeGenerator: Code generated in 432.40768 ms\n",
      "23/01/13 17:06:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 349.7 KiB, free 366.0 MiB)\n",
      "23/01/13 17:06:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 365.9 MiB)\n",
      "23/01/13 17:06:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.14:49250 (size: 33.9 KiB, free: 366.3 MiB)\n",
      "23/01/13 17:06:55 INFO SparkContext: Created broadcast 0 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "23/01/13 17:06:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/01/13 17:06:55 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "23/01/13 17:06:55 INFO DAGScheduler: Got job 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "23/01/13 17:06:55 INFO DAGScheduler: Final stage: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "23/01/13 17:06:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/01/13 17:06:55 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/13 17:06:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "23/01/13 17:06:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.6 KiB, free 365.9 MiB)\n",
      "23/01/13 17:06:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 365.9 MiB)\n",
      "23/01/13 17:06:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.14:49250 (size: 7.0 KiB, free: 366.3 MiB)\n",
      "23/01/13 17:06:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/13 17:06:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/13 17:06:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/01/13 17:06:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.14, executor driver, partition 0, PROCESS_LOCAL, 4933 bytes) taskResourceAssignments Map()\n",
      "23/01/13 17:06:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "23/01/13 17:06:56 INFO FileScanRDD: Reading File path: file:///Users/johnpaulbabu/Documents/DevOps/pyspark/y_list.txt, range: 0-50, partition values: [empty row]\n",
      "23/01/13 17:06:56 INFO CodeGenerator: Code generated in 31.231603 ms\n",
      "23/01/13 17:06:56 INFO CodeGenerator: Code generated in 8.451608 ms\n",
      "23/01/13 17:06:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1704 bytes result sent to driver\n",
      "23/01/13 17:06:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 840 ms on 192.168.0.14 (executor driver) (1/1)\n",
      "23/01/13 17:06:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/01/13 17:06:56 INFO DAGScheduler: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1.066 s\n",
      "23/01/13 17:06:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/01/13 17:06:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/01/13 17:06:56 INFO DAGScheduler: Job 0 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1.118817 s\n",
      "23/01/13 17:06:56 INFO CodeGenerator: Code generated in 11.323128 ms\n",
      "23/01/13 17:06:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.0 MiB, free 361.9 MiB)\n",
      "23/01/13 17:06:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 385.0 B, free 361.9 MiB)\n",
      "23/01/13 17:06:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.14:49250 (size: 385.0 B, free: 366.3 MiB)\n",
      "23/01/13 17:06:56 INFO SparkContext: Created broadcast 2 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "23/01/13 17:06:56 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/01/13 17:06:56 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/01/13 17:06:56 INFO FileSourceStrategy: Output Data Schema: struct<ID_x: string, value_x: string>\n",
      "23/01/13 17:06:56 INFO CodeGenerator: Code generated in 30.447035 ms\n",
      "23/01/13 17:06:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 349.7 KiB, free 361.6 MiB)\n",
      "23/01/13 17:06:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 361.5 MiB)\n",
      "23/01/13 17:06:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.14:49250 (size: 33.9 KiB, free: 366.2 MiB)\n",
      "23/01/13 17:06:56 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/13 17:06:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/01/13 17:06:57 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/13 17:06:57 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/13 17:06:57 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/01/13 17:06:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/01/13 17:06:57 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/13 17:06:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/13 17:06:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 15.7 KiB, free 361.5 MiB)\n",
      "23/01/13 17:06:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 361.5 MiB)\n",
      "23/01/13 17:06:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.14:49250 (size: 7.7 KiB, free: 366.2 MiB)\n",
      "23/01/13 17:06:57 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/13 17:06:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/13 17:06:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "23/01/13 17:06:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.14, executor driver, partition 0, PROCESS_LOCAL, 4933 bytes) taskResourceAssignments Map()\n",
      "23/01/13 17:06:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "23/01/13 17:06:57 INFO FileScanRDD: Reading File path: file:///Users/johnpaulbabu/Documents/DevOps/pyspark/x_list.txt, range: 0-50, partition values: [empty row]\n",
      "23/01/13 17:06:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1847 bytes result sent to driver\n",
      "23/01/13 17:06:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 70 ms on 192.168.0.14 (executor driver) (1/1)\n",
      "23/01/13 17:06:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "23/01/13 17:06:57 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.090 s\n",
      "23/01/13 17:06:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/01/13 17:06:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "23/01/13 17:06:57 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.100240 s\n",
      "23/01/13 17:06:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.14:49250 in memory (size: 7.0 KiB, free: 366.2 MiB)\n",
      "23/01/13 17:06:57 INFO CodeGenerator: Code generated in 26.597919 ms\n",
      "+----+-------+-------+\n",
      "|ID_x|value_x|value_y|\n",
      "+----+-------+-------+\n",
      "|  ID|   Word|   droW|\n",
      "|   A|  Hello|  olleH|\n",
      "|   B|  World|  dlroW|\n",
      "|   C|    How|    woH|\n",
      "|   D|     Do|     oD|\n",
      "|   E|    You|    uoY|\n",
      "|   F|     Do|     oD|\n",
      "|   G|      ?|      ?|\n",
      "+----+-------+-------+\n",
      "\n",
      "23/01/13 17:06:57 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "23/01/13 17:06:57 INFO SparkUI: Stopped Spark web UI at http://192.168.0.14:4045\n",
      "23/01/13 17:06:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "23/01/13 17:06:57 INFO MemoryStore: MemoryStore cleared\n",
      "23/01/13 17:06:57 INFO BlockManager: BlockManager stopped\n",
      "23/01/13 17:06:57 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "23/01/13 17:06:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "23/01/13 17:06:57 INFO SparkContext: Successfully stopped SparkContext\n",
      "23/01/13 17:06:57 INFO ShutdownHookManager: Shutdown hook called\n",
      "23/01/13 17:06:57 INFO ShutdownHookManager: Deleting directory /private/var/folders/8_/fxpby0zx7dl6sv253yrcxb0r0000gn/T/spark-e5561a76-7836-4206-ada2-bdd92119cb56\n",
      "23/01/13 17:06:57 INFO ShutdownHookManager: Deleting directory /private/var/folders/8_/fxpby0zx7dl6sv253yrcxb0r0000gn/T/spark-e5561a76-7836-4206-ada2-bdd92119cb56/pyspark-a5246319-6f56-42f9-a8c0-c1ee1097cdc0\n",
      "23/01/13 17:06:57 INFO ShutdownHookManager: Deleting directory /private/var/folders/8_/fxpby0zx7dl6sv253yrcxb0r0000gn/T/spark-9ff68cde-9f2f-4111-a72d-30755e5866b6\n"
     ]
    }
   ],
   "source": [
    "!spark-submit analysis.py --data_source_1 x_list.txt --data_source_2 y_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7b7a8-c36e-46f5-a663-4557057289d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
